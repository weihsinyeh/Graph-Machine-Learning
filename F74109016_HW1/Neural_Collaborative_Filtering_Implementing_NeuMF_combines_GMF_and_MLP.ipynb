{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0415bf40",
      "metadata": {
        "id": "0415bf40"
      },
      "source": [
        "# Neural Collaborative filtering\n",
        "Previosuly, we introduce the basics of collaborative filtering using item and user-based CF. <br>\n",
        "However, traditional CF cannot capture more general pattern due to the simplicity of its method.<br>\n",
        "Therefore, people resort to neural networks to learn more complex, fine-grained inforamtion from user-item interaction. <br>\n",
        "In this lecture, we will implement two classic NN-based CF model including: **BPR-MF** and **NeuMF**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e6fa5a",
      "metadata": {
        "id": "a3e6fa5a"
      },
      "source": [
        "## Data preparation\n",
        "Here we will use the most common, famous dataset: Movielens which contains the user-movie interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4748fb60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4748fb60",
        "outputId": "356f7d26-15ab-4cae-e278-e0d048c771da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b32ad5aa",
      "metadata": {
        "id": "b32ad5aa"
      },
      "outputs": [],
      "source": [
        "# import required modules\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.utils import structured_negative_sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44359c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b44359c2",
        "outputId": "f06838ff-cfdb-44f3-c84b-45ef11e04d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using existing file ml-latest-small.zip\n",
            "Extracting ./ml-latest-small.zip\n"
          ]
        }
      ],
      "source": [
        "# download the dataset\n",
        "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
        "extract_zip(download_url(url, '.'), '.')\n",
        "movie_path = './ml-latest-small/movies.csv'\n",
        "rating_path = './ml-latest-small/ratings.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1244d9",
      "metadata": {
        "id": "db1244d9"
      },
      "outputs": [],
      "source": [
        "# load user and movie nodes\n",
        "def load_mapping(path, index_col):\n",
        "    \"\"\"Loads csv containing interaction information\n",
        "\n",
        "    Args:\n",
        "        path (str): path to csv file\n",
        "        index_col (str): column name of index column\n",
        "\n",
        "    Returns:\n",
        "        dict: mapping of csv row to unique id\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, index_col=index_col)\n",
        "\n",
        "    # assign unique index for each user/movie\n",
        "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
        "    return mapping\n",
        "\n",
        "\n",
        "user_mapping = load_mapping(rating_path, index_col='userId')\n",
        "movie_mapping = load_mapping(movie_path, index_col='movieId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe36f0d5",
      "metadata": {
        "id": "fe36f0d5"
      },
      "outputs": [],
      "source": [
        "# load edges between users and movies\n",
        "def load_interaction(path, src_index_col, src_mapping, dst_index_col, dst_mapping, link_index_col, rating_threshold=4):\n",
        "    \"\"\"Loads csv containing edges between users and items\n",
        "\n",
        "    Args:\n",
        "        path (str): path to csv file\n",
        "        src_index_col (str): column name of users\n",
        "        src_mapping (dict): mapping between row number and user id\n",
        "        dst_index_col (str): column name of items\n",
        "        dst_mapping (dict): mapping between row number and item id\n",
        "        link_index_col (str): column name of user item interaction\n",
        "        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: N by 2 matrix containing the user item interaction\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    edge_index = None\n",
        "    src = [src_mapping[index] for index in df[src_index_col]]\n",
        "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
        "    is_positive = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
        "\n",
        "    interactions = []\n",
        "    for i in range(is_positive.shape[0]):\n",
        "        if is_positive[i]:\n",
        "            interactions.append([src[i],dst[i]])\n",
        "\n",
        "    return torch.tensor(interactions)\n",
        "\n",
        "\n",
        "interactions = load_interaction(\n",
        "    rating_path,\n",
        "    src_index_col='userId',\n",
        "    src_mapping=user_mapping,\n",
        "    dst_index_col='movieId',\n",
        "    dst_mapping=movie_mapping,\n",
        "    link_index_col='rating',\n",
        "    rating_threshold=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f11250d",
      "metadata": {
        "id": "3f11250d"
      },
      "outputs": [],
      "source": [
        "# split the edges of the graph using a 80/10/10 train/validation/test split\n",
        "num_users, num_movies = len(user_mapping), len(movie_mapping)\n",
        "num_interactions = interactions.shape[0]\n",
        "all_indices = [i for i in range(num_interactions)]\n",
        "\n",
        "train_indices, test_indices = train_test_split(\n",
        "    all_indices, test_size=0.2, random_state=1)\n",
        "val_indices, test_indices = train_test_split(\n",
        "    test_indices, test_size=0.5, random_state=1)\n",
        "\n",
        "train_interactions = interactions[train_indices,:]\n",
        "val_interactions = interactions[val_indices,:]\n",
        "test_interactions = interactions[test_indices,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c0ea70",
      "metadata": {
        "id": "03c0ea70"
      },
      "source": [
        "## The BPR-MF model\n",
        "BPR-MF is a matrix factorization model that factorize the user-item interaction matrix with user and item embedding matrix.\n",
        "![](https://i.imgur.com/FM1w89a.png)\n",
        "\n",
        "## Loss Function\n",
        "\n",
        "\n",
        "\n",
        "We utilize a Bayesian Personalized Ranking (BPR) loss, a pairwise objective which encourages the predictions of positive samples to be higher than negative samples for each user.\n",
        "\n",
        "\\begin{equation}\n",
        "L_{BPR} = -\\sum_{u = 1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln{\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})} + \\lambda ||E^{(0)}||^2\n",
        "\\end{equation}\n",
        "\n",
        "$\\hat{y}_{u}$: predicted score of a positive sample\n",
        "\n",
        "$\\hat{y}_{uj}$: predicted score of a negative sample\n",
        "\n",
        "$\\lambda$: hyperparameter which controls the L2 regularization strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad40619e",
      "metadata": {
        "id": "ad40619e"
      },
      "outputs": [],
      "source": [
        "class BPR(nn.Module):\n",
        "    def __init__(self, user_size, item_size, dim):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(user_size,dim)\n",
        "        self.item_embedding = nn.Embedding(item_size,dim)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        x_ui = torch.mul(self.user_embedding(u), self.item_embedding(i)).sum(dim=1)\n",
        "        x_uj = torch.mul(self.user_embedding(u), self.item_embedding(j)).sum(dim=1)\n",
        "        x_uij = x_ui - x_uj\n",
        "        bpr_loss = -torch.log(torch.sigmoid(x_uij)).mean()\n",
        "        return bpr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d4531c",
      "metadata": {
        "id": "38d4531c"
      },
      "outputs": [],
      "source": [
        "def bpr_loss(user_embedding, pos_embedding, neg_embedding):\n",
        "    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n",
        "\n",
        "    Args:\n",
        "        users_emb_final (torch.Tensor): e_u_k\n",
        "        pos_items_emb_final (torch.Tensor): positive e_i_k\n",
        "        neg_items_emb_final (torch.Tensor): negative e_i_k\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: scalar bpr loss value\n",
        "    \"\"\"\n",
        "\n",
        "    pos_scores = torch.mul(user_embedding, pos_embedding)\n",
        "    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n",
        "    neg_scores = torch.mul(user_embedding, neg_embedding)\n",
        "    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n",
        "\n",
        "    loss = -torch.mean(torch.sigmoid(pos_scores - neg_scores))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fed3a2a",
      "metadata": {
        "id": "9fed3a2a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TripletUniformPair(Dataset):\n",
        "    def __init__(self, num_item, user_list, pair):\n",
        "        self.num_item = num_item\n",
        "        self.user_list = user_list\n",
        "        self.pair = pair\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = np.random.randint(len(self.pair))\n",
        "        u = self.pair[idx][0]\n",
        "        i = self.pair[idx][1]\n",
        "        j = np.random.randint(self.num_item)\n",
        "        while j in self.user_list[u]:\n",
        "            j = np.random.randint(self.num_item)\n",
        "        return u, i, j\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b50a85c",
      "metadata": {
        "id": "1b50a85c"
      },
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "We evalaluate our model using the following metrics\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Recall} = \\frac{TP}{TP + FP}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Precision} = \\frac{TP}{TP + FN}\n",
        "\\end{equation}\n",
        "\n",
        "**Dicounted Cumulative Gain (DCG)** at rank position p is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{DCG}_\\text{p} = \\sum_{i = 1}^p \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n",
        "\\end{equation}\n",
        "\n",
        "p: a particular rank position\n",
        "\n",
        "$rel_i \\in \\{0, 1\\}$ : graded relevance of the result at position $i$\n",
        "\n",
        "**Idealised Dicounted Cumulative Gain (IDCG)**, namely the maximum possible DCG, at rank position $p$ is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{IDCG}_\\text{p} = \\sum_{i = 1}^{|REL_p|} \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n",
        "\\end{equation}\n",
        "\n",
        "$|REL_p|$ : list of items ordered by their relevance up to position p\n",
        "\n",
        "**Normalized Dicounted Cumulative Gain (NDCG)** at rank position $p$ is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{nDCG}_\\text{p} = \\frac{\\text{DCG}_p}{\\text{nDCG}_p}\n",
        "\\end{equation}\n",
        "\n",
        "Specifically, we use the metrics recall@K, precision@K, and NDCG@K. @K indicates that these metrics are computed on the top K recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fffb4e33",
      "metadata": {
        "id": "fffb4e33"
      },
      "outputs": [],
      "source": [
        "# helper function to get N_u\n",
        "def get_user_positive_items(interactions):\n",
        "    \"\"\"Generates dictionary of positive items for each user\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        dict: dictionary of positive items for each user\n",
        "    \"\"\"\n",
        "    user_pos_items = {}\n",
        "    for i in range(interactions.shape[0]):\n",
        "        user = interactions[i][0].item()\n",
        "        item = interactions[i][1].item()\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "        user_pos_items[user].append(item)\n",
        "    return user_pos_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe36cb8",
      "metadata": {
        "id": "8fe36cb8"
      },
      "outputs": [],
      "source": [
        "# computes recall@K and precision@K\n",
        "def RecallPrecision_ATk(groundTruth, r, k):\n",
        "    \"\"\"Computers recall @ k and precision @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (intg): determines the top k items to compute precision and recall on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k\n",
        "    \"\"\"\n",
        "    num_correct_pred = torch.sum(r, dim=-1)  # number of correctly predicted items per user\n",
        "    # number of items liked by each user in the test set\n",
        "    user_num_liked = torch.Tensor([len(groundTruth[i])\n",
        "                                  for i in range(len(groundTruth))])\n",
        "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
        "    precision = torch.mean(num_correct_pred) / k\n",
        "    return recall.item(), precision.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a08bd3",
      "metadata": {
        "id": "54a08bd3"
      },
      "outputs": [],
      "source": [
        "# computes NDCG@K\n",
        "def NDCGatK_r(groundTruth, r, k):\n",
        "    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (int): determines the top k items to compute ndcg on\n",
        "\n",
        "    Returns:\n",
        "        float: ndcg @ k\n",
        "    \"\"\"\n",
        "    assert len(r) == len(groundTruth)\n",
        "\n",
        "    test_matrix = torch.zeros((len(r), k))\n",
        "\n",
        "    for i, items in enumerate(groundTruth):\n",
        "        length = min(len(items), k)\n",
        "        test_matrix[i, :length] = 1\n",
        "    max_r = test_matrix\n",
        "    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n",
        "    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n",
        "    dcg = torch.sum(dcg, axis=1)\n",
        "    idcg[idcg == 0.] = 1.\n",
        "    ndcg = dcg / idcg\n",
        "    ndcg[torch.isnan(ndcg)] = 0.\n",
        "    return torch.mean(ndcg).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3fcb5d",
      "metadata": {
        "id": "eb3fcb5d"
      },
      "outputs": [],
      "source": [
        "# wrapper function to get evaluation metrics\n",
        "def get_metrics(model, interactions, exclude_interactions, k):\n",
        "    \"\"\"\n",
        "    Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
        "    \"\"\"\n",
        "    user_embedding, item_embedding = model.user_embedding.weight, model.item_embedding.weight\n",
        "\n",
        "    # get ratings between every user and item - shape is num users x num movies\n",
        "    rating = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    for exclude_record in exclude_interactions:\n",
        "        # gets all the positive items for each user from the edge index\n",
        "        user_pos_items = get_user_positive_items(exclude_record)\n",
        "        # get coordinates of all edges to exclude\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "\n",
        "        # set ratings of excluded edges to large negative value\n",
        "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
        "\n",
        "    # get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(rating, k=k)\n",
        "\n",
        "    # get all unique users in evaluated split\n",
        "    users = interactions[:,0].unique()\n",
        "\n",
        "    test_user_pos_items = get_user_positive_items(interactions)\n",
        "\n",
        "    # convert test user pos items dictionary into a list\n",
        "    test_user_pos_items_list = [\n",
        "        test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "    # determine the correctness of topk predictions\n",
        "    r = []\n",
        "    for user in users:\n",
        "        ground_truth_items = test_user_pos_items[user.item()]\n",
        "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
        "        r.append(label)\n",
        "    r = torch.Tensor(np.array(r).astype('float'))\n",
        "\n",
        "    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
        "    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
        "\n",
        "    return recall, precision, ndcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35863d6a",
      "metadata": {
        "id": "35863d6a"
      },
      "outputs": [],
      "source": [
        "# wrapper function to evaluate model\n",
        "def evaluation(model, edge_index, exclude_edge_indices, k):\n",
        "    \"\"\"\n",
        "    Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
        "    \"\"\"\n",
        "    # get embeddings\n",
        "    users_emb_final, items_emb_final = model.user_embedding.weight, model.item_embedding.weight\n",
        "    edges = structured_negative_sampling(\n",
        "        edge_index.T, contains_neg_self_loops=False)\n",
        "\n",
        "    # indices\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "    users_emb_final = users_emb_final[user_indices]\n",
        "    pos_items_emb_final = items_emb_final[pos_item_indices]\n",
        "    neg_items_emb_final = items_emb_final[neg_item_indices]\n",
        "\n",
        "    recall, precision, ndcg = get_metrics(\n",
        "        model, edge_index, exclude_edge_indices, k)\n",
        "\n",
        "    return recall, precision, ndcg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9e9e16",
      "metadata": {
        "id": "dd9e9e16"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef072d84",
      "metadata": {
        "id": "ef072d84"
      },
      "outputs": [],
      "source": [
        "# define contants\n",
        "ITERATIONS = 1000\n",
        "BATCH_SIZE = 1024\n",
        "LR = 1e-3\n",
        "ITERS_PER_EVAL = 100\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 1e-6\n",
        "DIM = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30fc7342",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30fc7342",
        "outputId": "db80c37b-364e-45d6-90f9-45267279af71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "model = BPR(num_users, num_movies, DIM)\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "# initialize parameters\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "user_interaction_list = get_user_positive_items(train_interactions)\n",
        "train_dataset = TripletUniformPair(num_movies, user_interaction_list, train_interactions.numpy())\n",
        "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,pin_memory=True,num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba816b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ba816b4",
        "outputId": "7fdfc36b-46ce-4ad0-ba2c-8a46c6e5dedb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-a835f6bef373>:1: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from tqdm.autonotebook import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3340ff8a",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "8968969a5825401196c8aa89f191394f",
            "a5498137207245b19129fa78744c74b4",
            "2b40654f4f0749e2a78df1d7ebae2d32",
            "3b5fd301675e459291a37ef16b3f03ab",
            "13cf9a980c194c31bbc86dbbd580e096",
            "3cf084083c0847cd84a2662f23913603",
            "a5fe476040754e1185c95775c6d99e57",
            "8232b6b808984d1b9df16ed18d1bc1c8",
            "7963747b14ed4efd83e713c041349515",
            "9665f05d420d4818945bdb78a0929061",
            "0c77af9d7c494904a6a94b2057bcadcc"
          ]
        },
        "id": "3340ff8a",
        "outputId": "f3a03949-779b-4015-a503-0249b7f1d7b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8968969a5825401196c8aa89f191394f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iteration 0/1000] val_recall@20: 0.0080, val_precision@20: 0.0028, val_ndcg@20: 0.0046\n",
            "[Iteration 100/1000] val_recall@20: 0.2116, val_precision@20: 0.0610, val_ndcg@20: 0.1444\n",
            "[Iteration 200/1000] val_recall@20: 0.2234, val_precision@20: 0.0637, val_ndcg@20: 0.1560\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5d9e0b27f5e5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_handling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_SIGCHLD_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_pids_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# prime the prefetch loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefetch_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker_queue_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mworker_queue_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_notempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_notempty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36m_start_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doing self._thread.start()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'... done self._thread.start()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0m_limbo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "for iter in trange(ITERATIONS):\n",
        "    for batch in train_loader:\n",
        "        batch = [item.to(device) for item in batch]\n",
        "        train_loss = model(*batch)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if iter % ITERS_PER_EVAL == 0:\n",
        "        model.eval()\n",
        "        recall, precision, ndcg = evaluation(model, val_interactions, [train_interactions], K)\n",
        "        print(f\"[Iteration {iter}/{ITERATIONS}] val_recall@{K}: {recall:.4f}, val_precision@{K}: {precision:.4f}, val_ndcg@{K}: {ndcg:.4f}\")\n",
        "        model.train()\n",
        "\n",
        "    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b8ecada",
      "metadata": {
        "id": "9b8ecada"
      },
      "outputs": [],
      "source": [
        "# evaluate on test set\n",
        "model.eval()\n",
        "\n",
        "test_recall, test_precision, test_ndcg = evaluation(\n",
        "            model, test_interactions, [train_interactions, val_interactions], K)\n",
        "\n",
        "print(f\"test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0e3c0a",
      "metadata": {
        "id": "3e0e3c0a"
      },
      "source": [
        "# Make New Recommendatios for a Given User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fdea58d",
      "metadata": {
        "id": "0fdea58d"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "df = pd.read_csv(movie_path)\n",
        "movieid_title = pd.Series(df.title.values,index=df.movieId).to_dict()\n",
        "movieid_genres = pd.Series(df.genres.values,index=df.movieId).to_dict()\n",
        "\n",
        "user_pos_items = get_user_positive_items(interactions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bbb1a3",
      "metadata": {
        "id": "33bbb1a3"
      },
      "outputs": [],
      "source": [
        "def make_predictions(user_id, num_recs):\n",
        "    user = user_mapping[user_id]\n",
        "    e_u = model.user_embedding.weight[user]\n",
        "    scores = model.item_embedding.weight @ e_u\n",
        "\n",
        "    values, indices = torch.topk(scores, k=len(user_pos_items[user]) + num_recs)\n",
        "\n",
        "    movies = [index.cpu().item() for index in indices if index in user_pos_items[user]][:num_recs]\n",
        "    movie_ids = [list(movie_mapping.keys())[list(movie_mapping.values()).index(movie)] for movie in movies]\n",
        "    titles = [movieid_title[id] for id in movie_ids]\n",
        "    genres = [movieid_genres[id] for id in movie_ids]\n",
        "\n",
        "    print(f\"Here are some movies that user {user_id} rated highly\")\n",
        "    for i in range(num_recs):\n",
        "        print(f\"title: {titles[i]}, genres: {genres[i]} \")\n",
        "\n",
        "    print()\n",
        "\n",
        "    movies = [index.cpu().item() for index in indices if index not in user_pos_items[user]][:num_recs]\n",
        "    movie_ids = [list(movie_mapping.keys())[list(movie_mapping.values()).index(movie)] for movie in movies]\n",
        "    titles = [movieid_title[id] for id in movie_ids]\n",
        "    genres = [movieid_genres[id] for id in movie_ids]\n",
        "\n",
        "    print(f\"Here are some suggested movies for user {user_id}\")\n",
        "    for i in range(num_recs):\n",
        "        print(f\"title: {titles[i]}, genres: {genres[i]} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e3f97c",
      "metadata": {
        "id": "17e3f97c"
      },
      "outputs": [],
      "source": [
        "USER_ID = 30\n",
        "NUM_RECS = 10\n",
        "\n",
        "make_predictions(USER_ID, NUM_RECS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf022b6",
      "metadata": {
        "id": "5bf022b6"
      },
      "source": [
        "# Colaborative filtering with NeuMF\n",
        "Neural Collaborative Filtering(NCF) replaces the user-item inner product with a neural architecture. By doing so NCF tried to achieve the following:\n",
        "\n",
        "* NCF tries to express and generalize MF under its framework.\n",
        "* NCF tries to learn User-item interactions through a multi-layer perceptron."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a7332c",
      "metadata": {
        "id": "94a7332c"
      },
      "source": [
        "## Generalize Matrix Factorization\n",
        "The predicted output of the GMF can be expressed as\n",
        "\n",
        "![](https://miro.medium.com/max/1196/1*oLGMj-8x7WLectRAk20ZVA.png)\n",
        "where\n",
        "* $a_{out}$: activation function\n",
        "* $h$: weights of the output layer\n",
        "![](https://miro.medium.com/max/1400/1*dwXGkFZCbzhRR0blfgQkVA.png)\n",
        "\n",
        "As you can see from the above table that GMF with identity activation function and edge weights as 1 is indeed MF. The other 2 variations are expansions on the generic MF. The last variation of GMF with sigmoid as activation is used in NCF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb29eda",
      "metadata": {
        "id": "1bb29eda"
      },
      "outputs": [],
      "source": [
        "class GMF(nn.Module):\n",
        "    def __init__(self, user_size, item_size, dim):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(user_size,dim)\n",
        "        self.item_embedding = nn.Embedding(item_size,dim)\n",
        "        self.output_layer = nn.Linear(dim,1,bias=False)\n",
        "\n",
        "    def forward(self, user_id, item_id):\n",
        "        user_vectors = self.user_embedding(user_id)\n",
        "        item_vectors = self.item_embedding(item_id)\n",
        "\n",
        "        # interaction\n",
        "        interactions = user_vectors * item_vectors\n",
        "\n",
        "        # predictions\n",
        "        predictions = self.output_layer(interactions)\n",
        "\n",
        "        return predictions.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "245d8c1a",
      "metadata": {
        "id": "245d8c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2066b27-23e8-445b-8294-951c2b36c859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
            "    reader_close()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
            "    reader_close()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n"
          ]
        }
      ],
      "source": [
        "import scipy.sparse as sp\n",
        "# load ratings as a dok matrix\n",
        "train_mat = sp.dok_matrix((num_users, num_movies), dtype=np.float32)\n",
        "for x in train_interactions:\n",
        "    train_mat[x[0], x[1]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c627c90b",
      "metadata": {
        "id": "c627c90b"
      },
      "outputs": [],
      "source": [
        "class NCFData(Dataset):\n",
        "    def __init__(self, features,\n",
        "                num_item, train_mat=None, num_ng=0, is_training=None):\n",
        "        super(NCFData, self).__init__()\n",
        "        \"\"\" Note that the labels are only useful when training, we thus\n",
        "            add them in the ng_sample() function.\n",
        "        \"\"\"\n",
        "        self.features_ps = features\n",
        "        self.num_item = num_item\n",
        "        self.train_mat = train_mat\n",
        "        self.num_ng = num_ng\n",
        "        self.is_training = is_training\n",
        "        self.labels = [0 for _ in range(len(features))]\n",
        "\n",
        "    def ng_sample(self):\n",
        "        assert self.is_training, 'no need to sampling when testing'\n",
        "\n",
        "        self.features_ng = []\n",
        "        for x in self.features_ps:\n",
        "            u = x[0]\n",
        "            for t in range(self.num_ng):\n",
        "                j = np.random.randint(self.num_item)\n",
        "                while (u, j) in self.train_mat:\n",
        "                    j = np.random.randint(self.num_item)\n",
        "                self.features_ng.append([u, j])\n",
        "\n",
        "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
        "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
        "\n",
        "        self.features_fill = self.features_ps + self.features_ng\n",
        "        self.labels_fill = labels_ps + labels_ng\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.num_ng + 1) * len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = self.features_fill if self.is_training \\\n",
        "                    else self.features_ps\n",
        "        labels = self.labels_fill if self.is_training \\\n",
        "                    else self.labels\n",
        "\n",
        "        user = features[idx][0]\n",
        "        item = features[idx][1]\n",
        "        label = labels[idx]\n",
        "        return user, item ,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd41d0b",
      "metadata": {
        "id": "afd41d0b"
      },
      "outputs": [],
      "source": [
        "def hit(gt_item, pred_items):\n",
        "    if gt_item in pred_items:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "    if gt_item in pred_items:\n",
        "        index = pred_items.index(gt_item)\n",
        "        return np.reciprocal(np.log2(index+2))\n",
        "    return 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k):\n",
        "    HR, NDCG = [], []\n",
        "\n",
        "    for user, item, label in test_loader:\n",
        "        user = user.cuda()\n",
        "        item = item.cuda()\n",
        "\n",
        "        predictions = model(user, item)\n",
        "        _, indices = torch.topk(predictions, top_k)\n",
        "        recommends = torch.take(\n",
        "                item, indices).cpu().numpy().tolist()\n",
        "\n",
        "        gt_item = item[0].item()\n",
        "        HR.append(hit(gt_item, recommends))\n",
        "        NDCG.append(ndcg(gt_item, recommends))\n",
        "\n",
        "    return np.mean(HR), np.mean(NDCG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bdf8575",
      "metadata": {
        "id": "5bdf8575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e5d059-dc32-4cbf-a3d1-26e47fc6bfaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# construct the train and test datasets\n",
        "train_dataset = NCFData(\n",
        "    train_interactions.tolist(), num_movies, train_mat, 0, True)\n",
        "test_dataset = NCFData(\n",
        "        test_interactions.tolist(), num_movies, train_mat, 0, False)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset,\n",
        "        batch_size=100, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a0096e",
      "metadata": {
        "id": "18a0096e"
      },
      "outputs": [],
      "source": [
        "# define contants\n",
        "ITERATIONS = 50\n",
        "BATCH_SIZE = 256\n",
        "LR = 1e-4\n",
        "ITERS_PER_EVAL = 100\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 1e-6\n",
        "DIM = 32\n",
        "\n",
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "model = GMF(num_users, num_movies, DIM)\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "# initialize parameters\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "objective_function = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14cb842",
      "metadata": {
        "scrolled": true,
        "id": "c14cb842"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "for iter in trange(ITERATIONS):\n",
        "    train_loader.dataset.ng_sample()\n",
        "    for batch in train_loader:\n",
        "        batch = [item.to(device) for item in batch]\n",
        "        user, item, label = batch\n",
        "        predictions = model(user,item)\n",
        "        loss = objective_function(predictions,label.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "    HR, NDCG = metrics(model, test_loader, K)\n",
        "    print(\"HR@{}: {:.3f}  NDCG@{}: {:.3f}\".format(K,np.mean(HR),K, np.mean(NDCG)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d4129fe",
      "metadata": {
        "id": "4d4129fe"
      },
      "source": [
        "## MLP in NCF\n",
        "NCF is an example of multimodal deep learning as it contains data from 2 pathways namely user and item. The most intuitive way to combine them is by concatenation. But a simple vector concatenation does not account for user-item interactions and is insufficient to model the collaborative filtering effect. To address this NCF adds hidden layers on top of concatenated user-item vectors(MLP framework), to learn user-item interactions. This endows the model with a lot of flexibility and non-linearity to learn the user-item interactions. This is an upgrade over MF that uses a fixed element-wise product on them. More precisely, the MLP alter Equation 1 as follows\n",
        "![](https://miro.medium.com/max/1252/1*tIQfBeTur0gaKObfvDXmpw.png)\n",
        "![](https://miro.medium.com/max/1400/1*aP-Mx266ExwoWZPSdHtYpA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810ecaa1",
      "metadata": {
        "id": "810ecaa1"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, user_size, item_size, dim):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(user_size,dim)\n",
        "        self.item_embedding = nn.Embedding(item_size,dim)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(dim*2,dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim,1),\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, item_id):\n",
        "        user_vectors = self.user_embedding(user_id)\n",
        "        item_vectors = self.item_embedding(item_id)\n",
        "\n",
        "        # interaction\n",
        "        interactions = torch.cat([user_vectors, item_vectors],dim=-1)\n",
        "\n",
        "        # predictions\n",
        "        predictions = self.MLP(interactions)\n",
        "\n",
        "        return predictions.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fce51ff",
      "metadata": {
        "id": "7fce51ff"
      },
      "outputs": [],
      "source": [
        "# define contants\n",
        "ITERATIONS = 50\n",
        "BATCH_SIZE = 256\n",
        "LR = 1e-4\n",
        "ITERS_PER_EVAL = 100\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 1e-6\n",
        "DIM = 32\n",
        "\n",
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "model = MLP(num_users, num_movies, DIM)\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "# initialize parameters\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "objective_function = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40cec0fc",
      "metadata": {
        "scrolled": true,
        "id": "40cec0fc"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "for iter in trange(ITERATIONS):\n",
        "    train_loader.dataset.ng_sample()\n",
        "    for batch in train_loader:\n",
        "        batch = [item.to(device) for item in batch]\n",
        "        user, item, label = batch\n",
        "        predictions = model(user,item)\n",
        "        loss = objective_function(predictions,label.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "    HR, NDCG = metrics(model, test_loader, K)\n",
        "    print(\"HR@{}: {:.3f}  NDCG@{}: {:.3f}\".format(K,np.mean(HR),K, np.mean(NDCG)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2519c97",
      "metadata": {
        "id": "e2519c97"
      },
      "source": [
        "# Practice: implementing NeuMF that combines GMF and MLP\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*Tqk7Q2q7wsr6MLF8Xl-emg.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuMF(nn.Module):\n",
        "    def __init__(self, user_size, item_size, dim):\n",
        "        super().__init__()\n",
        "        ############################################################################\n",
        "        # TODO: Your code here!\n",
        "        # create embeddings and layers from GMF and MLP\n",
        "\n",
        "        ############################################################################\n",
        "        # an output layer that transform concat vector to prediction\n",
        "        self.user_embedding = nn.Embedding(user_size,dim)\n",
        "        self.item_embedding = nn.Embedding(item_size,dim)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(2*dim,dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim,dim),\n",
        "            #nn.ReLU(),\n",
        "            #nn.Linear(dim,1),\n",
        "        )\n",
        "        self.GMF = nn.Linear(2*dim,dim,bias=False)\n",
        "        self.output_layer = nn.Linear(dim*2,1,bias=False)\n",
        "\n",
        "    def forward(self, user_id, item_id):\n",
        "        ############################################################################\n",
        "        # TODO: Your code here!\n",
        "        # please obtain the embedding from MLP and GMF\n",
        "        # store the embedding in MLP_feature and GMF_feature, respectively.\n",
        "\n",
        "        ############################################################################\n",
        "        user_vectors = self.user_embedding(user_id)\n",
        "        item_vectors = self.item_embedding(item_id)\n",
        "\n",
        "        # interaction\n",
        "        interactions = torch.cat([user_vectors, item_vectors],dim=-1)\n",
        "\n",
        "        MLP_feature = self.MLP(interactions)\n",
        "\n",
        "        GMF_feature = self.GMF(interactions)\n",
        "        # predictions\n",
        "        features = torch.cat([MLP_feature,GMF_feature],dim=-1)\n",
        "        predictions = self.output_layer(features)\n",
        "\n",
        "        return predictions.flatten()"
      ],
      "metadata": {
        "id": "1IroDkPW-Z0u"
      },
      "id": "1IroDkPW-Z0u",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "889675ba",
      "metadata": {
        "id": "889675ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2acf51-7409-4363-dd74-a39d67d14816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu.\n"
          ]
        }
      ],
      "source": [
        "# define contants\n",
        "ITERATIONS = 50\n",
        "BATCH_SIZE = 256\n",
        "LR = 1e-4\n",
        "ITERS_PER_EVAL = 100\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 0\n",
        "DIM = 32\n",
        "\n",
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "model = NeuMF(num_users, num_movies, DIM).to(device)\n",
        "model.train()\n",
        "\n",
        "# initialize parameters\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "objective_function = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6451c1",
      "metadata": {
        "id": "ca6451c1"
      },
      "source": [
        "## Training!\n",
        "Let's see if NeuMF outperforms GMF and MLP?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "27e3bf88",
      "metadata": {
        "scrolled": true,
        "id": "27e3bf88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cea0804357b0403d81504ce4c4da44b3",
            "bca588cbb2b649319ad4e1599cb87ef1",
            "66b46c37dc164a998720f4ee28c5a404",
            "17f2a6a6e0d4488698d2d0d37882ff76",
            "0d5cbef856c242be831ee78a7b988697",
            "525c41d8e85e4d4487ce0f5fd77af320",
            "bbb01bc921e545909047b16948561ec0",
            "1152bb965077478ca98fb358b3a5083a",
            "f514766b0d9540709ab01b1699c71890",
            "16e2008a019e4f4a833a3959741c3421",
            "057876525d7948c59274fff9e79ed4f5"
          ]
        },
        "outputId": "badb4fb7-5d22-450f-dad0-a5de849f27b1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cea0804357b0403d81504ce4c4da44b3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# training loop\n",
        "for iter in trange(ITERATIONS):\n",
        "    train_loader.dataset.ng_sample()\n",
        "    for batch in train_loader:\n",
        "        batch = [item.to(device) for item in batch]\n",
        "        user, item, label = batch\n",
        "        predictions = model(user,item)\n",
        "        loss = objective_function(predictions,label.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "    #HR, NDCG = metrics(model, test_loader, K)\n",
        "    #print(\"HR@{}: {:.3f}  NDCG@{}: {:.3f}\".format(K,np.mean(HR),K, np.mean(NDCG)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyg",
      "language": "python",
      "name": "pyg"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8968969a5825401196c8aa89f191394f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5498137207245b19129fa78744c74b4",
              "IPY_MODEL_2b40654f4f0749e2a78df1d7ebae2d32",
              "IPY_MODEL_3b5fd301675e459291a37ef16b3f03ab"
            ],
            "layout": "IPY_MODEL_13cf9a980c194c31bbc86dbbd580e096"
          }
        },
        "a5498137207245b19129fa78744c74b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cf084083c0847cd84a2662f23913603",
            "placeholder": "",
            "style": "IPY_MODEL_a5fe476040754e1185c95775c6d99e57",
            "value": " 21%"
          }
        },
        "2b40654f4f0749e2a78df1d7ebae2d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8232b6b808984d1b9df16ed18d1bc1c8",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7963747b14ed4efd83e713c041349515",
            "value": 207
          }
        },
        "3b5fd301675e459291a37ef16b3f03ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9665f05d420d4818945bdb78a0929061",
            "placeholder": "",
            "style": "IPY_MODEL_0c77af9d7c494904a6a94b2057bcadcc",
            "value": " 207/1000 [05:33&lt;19:43,  1.49s/it]"
          }
        },
        "13cf9a980c194c31bbc86dbbd580e096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf084083c0847cd84a2662f23913603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5fe476040754e1185c95775c6d99e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8232b6b808984d1b9df16ed18d1bc1c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7963747b14ed4efd83e713c041349515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9665f05d420d4818945bdb78a0929061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c77af9d7c494904a6a94b2057bcadcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cea0804357b0403d81504ce4c4da44b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bca588cbb2b649319ad4e1599cb87ef1",
              "IPY_MODEL_66b46c37dc164a998720f4ee28c5a404",
              "IPY_MODEL_17f2a6a6e0d4488698d2d0d37882ff76"
            ],
            "layout": "IPY_MODEL_0d5cbef856c242be831ee78a7b988697"
          }
        },
        "bca588cbb2b649319ad4e1599cb87ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_525c41d8e85e4d4487ce0f5fd77af320",
            "placeholder": "",
            "style": "IPY_MODEL_bbb01bc921e545909047b16948561ec0",
            "value": "100%"
          }
        },
        "66b46c37dc164a998720f4ee28c5a404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1152bb965077478ca98fb358b3a5083a",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f514766b0d9540709ab01b1699c71890",
            "value": 50
          }
        },
        "17f2a6a6e0d4488698d2d0d37882ff76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16e2008a019e4f4a833a3959741c3421",
            "placeholder": "",
            "style": "IPY_MODEL_057876525d7948c59274fff9e79ed4f5",
            "value": " 50/50 [01:29&lt;00:00,  1.64s/it]"
          }
        },
        "0d5cbef856c242be831ee78a7b988697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "525c41d8e85e4d4487ce0f5fd77af320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb01bc921e545909047b16948561ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1152bb965077478ca98fb358b3a5083a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f514766b0d9540709ab01b1699c71890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16e2008a019e4f4a833a3959741c3421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057876525d7948c59274fff9e79ed4f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}